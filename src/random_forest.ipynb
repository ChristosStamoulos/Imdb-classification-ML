{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ανάλυση IMDB δεδομένων\n",
    "### Random Forest Ταξινομητής\n",
    "\n",
    "Ορίζουμε τις **υπερπαραμέτρους** που θα χρησιμοποιηθούν για την ανάπλαση των δωσμένων δεδομένων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3000   # Πλήθος λέξεων του λεξιλογίου\n",
    "n = 50     # Πιο συχνές λέξεις για παράληψη\n",
    "k = 0      # Λιγότερο συχνές λέξεις για παράληψη"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αντλούμε τα δεδομένα από τη βάση δεδομένων IMDB, αγνοώντας τις πιο συχνά χρησιμοποιούμενες λέξεις n και τις λιγότερες χρησιμοποιούμενες λέξεις k. (υπερπαράμετροι)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from math import log \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=m-k, skip_top=n)\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n",
    "\n",
    "x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
    "x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Δημιουργία δυαδικών διανυσμάτων"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μετατρέπουμε τα αντλημένα δεδομένα σε δυαδικά διανύσματα. Οι δυνατές τιμές είναι 0 και 1. Τιμή 1 συνεπάγεται ότι η αντίστοιχη λέξη περιέχεται στο κείμενο, ενώ τιμή 0 συνεπάγεται ότι δεν περιέχεται."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2893\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "x_train_binary = binary_vectorizer.fit_transform(x_train)\n",
    "x_test_binary = binary_vectorizer.transform(x_test)\n",
    "\n",
    "x_train_binary = np.array(x_train_binary.toarray())\n",
    "x_test_binary = np.array(x_test_binary.toarray())\n",
    "print(\n",
    "    'Vocabulary size:', len(binary_vectorizer.vocabulary_)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χ είναι ένας πίνακας διανυσμάτων (αξιολογήσεων), με κάθε διάνυσμα να αναπαριστά λέξεις. Y είναι ένα διάνυσμα με ετικέτες (0 ή 1) όπου 0 είναι μια αρνητική αξιολόγηση και 1 μια θετική."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X = \\begin{bmatrix} \\vec{x_{1}} \\\\ \\vdots \\\\ \\vec{x_{m}} \\end{bmatrix}\\, \\, \\, \n",
    "y = \\begin{bmatrix} y_{1} \\\\ \\vdots \\\\ y_{m} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Υλοποίηση του Random Forest Ταξινομητή"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Κλάση που υλοποιεί τον Random Forest Ταξινομητή. Η κλαση αποτελείται από δύο βασικές μεθόδους, την fit και την predict. Η μέθοδος fit εκπαιδεύει τον αλγόριθμο χρησιμόποιώντας τον πίνακα των δυαδικών διανυσμάτων X και τον πίνακα με τις ετικέτες y. Η μέθοδος predict δέχεται ένα πίνακα διανυσμάτων και επιστρέφει ένα προβλεπόμενο διάνυσμα με ετικέτες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "import numpy as np\n",
    "from id3test import ID3\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, m=500, num_of_trees=3, max_depth=10):\n",
    "        self.m = m\n",
    "        self.num_of_trees = num_of_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.random_trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.random_trees.clear()\n",
    "        print(\"Starting fitting process\")\n",
    "\n",
    "        for i in range(self.num_of_trees):\n",
    "            print(f\"Fitting tree {i + 1}/{self.num_of_trees}\")\n",
    "            random_x, random_y = self.select_random_samples(X, y)\n",
    "\n",
    "            if len(random_x) > 0:\n",
    "                print(f\"Random samples: {random_x.shape}\")\n",
    "                random_features = self.select_random_features(random_x)\n",
    "                id3 = ID3(random_features, 10)\n",
    "                tree = id3.fit(random_x, random_y)\n",
    "                \n",
    "                if tree is not None:  # Check if a valid tree is returned\n",
    "                    print(f\"Tree {i + 1} fitted successfully\")\n",
    "                    self.random_trees.append(id3)\n",
    "                else:\n",
    "                    print(f\"Skipping tree {i + 1} due to empty data or other issues.\")\n",
    "            else:\n",
    "                print(f\"Skipping tree {i + 1} due to empty data.\")\n",
    "\n",
    "        print(\"Fitting process completed\")\n",
    "\n",
    "    def select_random_samples(self, X, y):\n",
    "        x_sample = list()\n",
    "        y_sample = list()\n",
    "        indices = np.arange(len(y))\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            random_choice = np.random.choice(indices)\n",
    "            x_sample.append(X[random_choice])\n",
    "            y_sample.append(y[random_choice])\n",
    "\n",
    "        x_sample = np.array(x_sample)\n",
    "        y_sample = np.array(y_sample)   \n",
    "\n",
    "        return x_sample, y_sample\n",
    "\n",
    "    def select_random_features(self, random_x):\n",
    "        indices = np.arange(len(random_x[0]))\n",
    "        random_feature_indices = np.random.choice(indices, self.m, replace=False)\n",
    "\n",
    "        return random_feature_indices\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        total = list()\n",
    "        \n",
    "        for tree in self.random_trees:\n",
    "            categories = tree.predict(X)\n",
    "            total.append(np.array(categories))\n",
    "\n",
    "        for i in range(0, X.shape[0]):################ \n",
    "            num1 = 0\n",
    "            num0 = 0\n",
    "            for category in total:\n",
    "                if(category[i] == 1):\n",
    "                    num1 +=1\n",
    "                else:\n",
    "                    num0 +=1\n",
    "            if num1 > num0:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting process\n",
      "Fitting tree 1/1\n",
      "Random samples: (5000, 1906)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DecisionTree.__init__() got an unexpected keyword argument 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRandomForest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_of_trees\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_binary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test_binary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_predictions\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m      4\u001b[0m data_table \u001b[38;5;241m=\u001b[39m classification_table(data, x_train_binary)\n",
      "File \u001b[1;32mc:\\Users\\shina\\Desktop\\Πανεπιστήμιο\\3ο Έτος\\5ο Εξάμηνο\\Τεχνητή Νοημοσύνη\\Εργασίες\\Εργασία 2\\Imdb-classification-ML\\src\\visualizations.py:30\u001b[0m, in \u001b[0;36mclassification_data\u001b[1;34m(classifier, x_train, y_train, x_test, y_test, splits, model)\u001b[0m\n\u001b[0;32m     27\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((y_train, y_splits[i]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Εκπαίδευση του μοντέλου και λήψη προβλέψεων εκπαίδευσης/ελέγχου\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m train_pred \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(x_train)\n\u001b[0;32m     32\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m, in \u001b[0;36mRandomForest.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m random_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_random_features(random_x)\n\u001b[1;32m---> 23\u001b[0m id3 \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_depth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m tree \u001b[38;5;241m=\u001b[39m id3\u001b[38;5;241m.\u001b[39mfit(random_x, random_y)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tree \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Check if a valid tree is returned\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: DecisionTree.__init__() got an unexpected keyword argument 'features'"
     ]
    }
   ],
   "source": [
    "data = classification_data(RandomForest(200, num_of_trees = 1), x_train_binary, y_train, x_test_binary, y_test, 5)\n",
    "\n",
    "print(classification_report(y_test, data['test_predictions']))\n",
    "data_table = classification_table(data, x_train_binary)\n",
    "ipd.display(data_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fitting process\n",
      "Fitting tree 1/3\n",
      "Random samples: (25000, 2893)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ID3.create_tree() missing 1 required positional argument: 'max_depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForest(\u001b[38;5;241m200\u001b[39m, num_of_trees \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_binary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_train, rf\u001b[38;5;241m.\u001b[39mpredict(x_train_binary),\n\u001b[0;32m      4\u001b[0m                             zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, rf\u001b[38;5;241m.\u001b[39mpredict(x_test_binary),\n\u001b[0;32m      6\u001b[0m                             zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[1;32mIn[4], line 24\u001b[0m, in \u001b[0;36mRandomForest.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     22\u001b[0m random_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_random_features(random_x)\n\u001b[0;32m     23\u001b[0m id3 \u001b[38;5;241m=\u001b[39m ID3(random_features, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[43mid3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tree \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Check if a valid tree is returned\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTree \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fitted successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shina\\Desktop\\Πανεπιστήμιο\\3ο Έτος\\5ο Εξάμηνο\\Τεχνητή Νοημοσύνη\\Εργασίες\\Εργασία 2\\Imdb-classification-ML\\src\\id3test.py:24\u001b[0m, in \u001b[0;36mID3.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03mcreates the tree\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     23\u001b[0m most_common \u001b[38;5;241m=\u001b[39m mode(y\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmost_common\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree\n",
      "File \u001b[1;32mc:\\Users\\shina\\Desktop\\Πανεπιστήμιο\\3ο Έτος\\5ο Εξάμηνο\\Τεχνητή Νοημοσύνη\\Εργασίες\\Εργασία 2\\Imdb-classification-ML\\src\\id3test.py:63\u001b[0m, in \u001b[0;36mID3.create_tree\u001b[1;34m(self, x_train, y_train, features, max_depth, category, depth)\u001b[0m\n\u001b[0;32m     59\u001b[0m y_train_1 \u001b[38;5;241m=\u001b[39m y_train[x_train[:, max_ig_idx] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     61\u001b[0m new_features_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(features\u001b[38;5;241m.\u001b[39mflatten(), max_ig_idx)  \u001b[38;5;66;03m# remove current feature\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m root\u001b[38;5;241m.\u001b[39mleft_child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_features_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# go left for X = 1\u001b[39;00m\n\u001b[0;32m     66\u001b[0m root\u001b[38;5;241m.\u001b[39mright_child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_tree(x_train\u001b[38;5;241m=\u001b[39mx_train_0, y_train\u001b[38;5;241m=\u001b[39my_train_0, features\u001b[38;5;241m=\u001b[39mnew_features_indices,\n\u001b[0;32m     67\u001b[0m                                     category\u001b[38;5;241m=\u001b[39mm)  \u001b[38;5;66;03m# go right for X = 0\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m root\n",
      "\u001b[1;31mTypeError\u001b[0m: ID3.create_tree() missing 1 required positional argument: 'max_depth'"
     ]
    }
   ],
   "source": [
    "rf = RandomForest(200, num_of_trees = 3, max_depth=100)\n",
    "rf.fit(x_train_binary, y_train)\n",
    "print(classification_report(y_train, rf.predict(x_train_binary),\n",
    "                            zero_division=1))\n",
    "print(classification_report(y_test, rf.predict(x_test_binary),\n",
    "                            zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95     12500\n",
      "           1       0.95      0.95      0.95     12500\n",
      "\n",
      "    accuracy                           0.95     25000\n",
      "   macro avg       0.95      0.95      0.95     25000\n",
      "weighted avg       0.95      0.95      0.95     25000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.73      0.74     12500\n",
      "           1       0.74      0.75      0.74     12500\n",
      "\n",
      "    accuracy                           0.74     25000\n",
      "   macro avg       0.74      0.74      0.74     25000\n",
      "weighted avg       0.74      0.74      0.74     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(criterion='entropy', max_features=200, n_estimators=3, max_depth=100)\n",
    "rf.fit(x_train_binary, y_train)\n",
    "print(classification_report(y_train, rf.predict(x_train_binary),\n",
    "                            zero_division=1))\n",
    "print(classification_report(y_test, rf.predict(x_test_binary),\n",
    "                            zero_division=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
