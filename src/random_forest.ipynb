{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ανάλυση IMDB δεδομένων\n",
    "### Random Forest Ταξινομητής\n",
    "\n",
    "Ορίζουμε τις **υπερπαραμέτρους** που θα χρησιμοποιηθούν για την ανάπλαση των δωσμένων δεδομένων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3000   # Πλήθος λέξεων του λεξιλογίου\n",
    "n = 50     # Πιο συχνές λέξεις για παράληψη\n",
    "k = 0      # Λιγότερο συχνές λέξεις για παράληψη"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αντλούμε τα δεδομένα από τη βάση δεδομένων IMDB, αγνοώντας τις πιο συχνά χρησιμοποιούμενες λέξεις n και τις λιγότερες χρησιμοποιούμενες λέξεις k. (υπερπαράμετροι)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from math import log \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=m-k, skip_top=n)\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n",
    "\n",
    "x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
    "x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Δημιουργία δυαδικών διανυσμάτων"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μετατρέπουμε τα αντλημένα δεδομένα σε δυαδικά διανύσματα. Οι δυνατές τιμές είναι 0 και 1. Τιμή 1 συνεπάγεται ότι η αντίστοιχη λέξη περιέχεται στο κείμενο, ενώ τιμή 0 συνεπάγεται ότι δεν περιέχεται."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2893\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "x_train_binary = binary_vectorizer.fit_transform(x_train)\n",
    "x_test_binary = binary_vectorizer.transform(x_test)\n",
    "\n",
    "x_train_binary = np.array(x_train_binary.toarray())\n",
    "x_test_binary = np.array(x_test_binary.toarray())\n",
    "print(\n",
    "    'Vocabulary size:', len(binary_vectorizer.vocabulary_)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χ είναι ένας πίνακας διανυσμάτων (αξιολογήσεων), με κάθε διάνυσμα να αναπαριστά λέξεις. Y είναι ένα διάνυσμα με ετικέτες (0 ή 1) όπου 0 είναι μια αρνητική αξιολόγηση και 1 μια θετική."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X = \\begin{bmatrix} \\vec{x_{1}} \\\\ \\vdots \\\\ \\vec{x_{m}} \\end{bmatrix}\\, \\, \\, \n",
    "y = \\begin{bmatrix} y_{1} \\\\ \\vdots \\\\ y_{m} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Υλοποίηση του Random Forest Ταξινομητή"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Κλάση που υλοποιεί τον Random Forest Ταξινομητή. Η κλαση αποτελείται από δύο βασικές μεθόδους, την fit και την predict. Η μέθοδος fit εκπαιδεύει τον αλγόριθμο χρησιμόποιώντας τον πίνακα των δυαδικών διανυσμάτων X και τον πίνακα με τις ετικέτες y. Η μέθοδος predict δέχεται ένα πίνακα διανυσμάτων και επιστρέφει ένα προβλεπόμενο διάνυσμα με ετικέτες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "import numpy as np\n",
    "from id3 import ID3\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, m=200, num_of_trees=3, max_depth=10):\n",
    "        self.m = m\n",
    "        self.num_of_trees = num_of_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.random_trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.random_trees.clear()\n",
    "        print(\"Starting fitting process\")\n",
    "\n",
    "        for i in range(self.num_of_trees):\n",
    "            print(f\"Fitting tree {i + 1}/{self.num_of_trees}\")\n",
    "            random_x, random_y = self.select_random_samples(X, y)\n",
    "\n",
    "            if len(random_x) > 0:\n",
    "                print(f\"Random samples: {random_x.shape}\")\n",
    "                random_features = self.select_random_features(random_x)\n",
    "                id3 = ID3(features=random_features, max_depth=10)\n",
    "                tree = id3.fit(random_x, random_y)\n",
    "                \n",
    "                if tree is not None:  # Check if a valid tree is returned\n",
    "                    print(f\"Tree {i + 1} fitted successfully\")\n",
    "                    self.random_trees.append(id3)\n",
    "                else:\n",
    "                    print(f\"Skipping tree {i + 1} due to empty data or other issues.\")\n",
    "            else:\n",
    "                print(f\"Skipping tree {i + 1} due to empty data.\")\n",
    "\n",
    "        print(\"Fitting process completed\")\n",
    "\n",
    "    def select_random_samples(self, X, y):\n",
    "        x_sample = list()\n",
    "        y_sample = list()\n",
    "        indices = np.arange(len(y))\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            random_choice = np.random.choice(indices)\n",
    "            x_sample.append(X[random_choice])\n",
    "            y_sample.append(y[random_choice])\n",
    "\n",
    "        x_sample = np.array(x_sample)\n",
    "        y_sample = np.array(y_sample)   \n",
    "\n",
    "        return x_sample, y_sample\n",
    "\n",
    "    def select_random_features(self, random_x):\n",
    "        indices = np.arange(len(random_x[0]))\n",
    "        random_feature_indices = np.random.choice(indices, self.m, replace=False)\n",
    "\n",
    "        return random_feature_indices\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        total = list()\n",
    "        \n",
    "        for tree in self.random_trees:\n",
    "            categories = tree.predict(X)\n",
    "            total.append(np.array(categories))\n",
    "\n",
    "        for i in range(0, X.shape[0]):################ \n",
    "            num1 = 0\n",
    "            num0 = 0\n",
    "            for category in total:\n",
    "                if(category[i] == 1):\n",
    "                    num1 +=1\n",
    "                else:\n",
    "                    num0 +=1\n",
    "            if num1 > num0:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Αποτελέσματα του Random Forest Ταξινομητή\n",
    "\n",
    "Ο Random Forest Classifier αξιολογείται στα στα υπάρχοντα datasets και παράγει για κάθε υποσύνολο δεδομένων αποκρίσεις εκπαίδευσης και αξιολόγισης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = classification_data(RandomForest(m=200, num_of_trees = 3, max_depth=10), x_train_binary, y_train, x_test_binary, y_test, 5)\n",
    "\n",
    "print(classification_report(y_test, data['test_predictions']))\n",
    "data_table = classification_table(data, x_train_binary)\n",
    "ipd.display(data_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Καμπύλη Μάθησης του Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Καμπύλες ακρίβειας, προσεγγιστικής ακρίβειας, ανάκλησης και F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_plots(data)\n",
    "r = classification_plots(data, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Σύγκριση Random Forest και SKLearn's Random Forest\n",
    "\n",
    "Ο Random Forest αξιολογείται ενάντια στον Random Forest Classifier της βιβλιοθήκης SKLearn. Τα αποτελέσματα είναι πανομοιότυπα μεταξύ τους. Παρακάτω παρουσιάζεται ο πίνακας διαφοράς των αποτελεσμάτων των 2 αλγορίθμων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(criterion='entropy', max_features=200, n_estimators=3, max_depth=10)\n",
    "rf_data = classification_data(rf, x_train_binary, y_train, x_test_binary, y_test, 5)\n",
    "rf_data['estimator'] = \"SKLearn's Random Forest\"\n",
    "rf_table = classification_table(rf_data, x_train_binary)\n",
    "difference_table = abs(rf_table - data_table)\n",
    "difference_table = difference_table.style.set_caption('Πίνακας Διαφοράς για τον {estimator} και τον {estimator_2}'.format(estimator=data['estimator'], estimator_2=rf_data['estimator']))\n",
    "ipd.display(difference_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(rf_data)\n",
    "classification_plots(rf_data)\n",
    "r = classification_plots_compare(data, rf_data, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Σύγκριση Random Forest και SKLearn's Ada Boost\n",
    "\n",
    "Ο Random Forest αξιολογείται ενάντια στον Ada Boost της βιβλιοθήκης SKLearn. Τα αποτελέσματα είναι πανομοιότυπα μεταξύ τους. Παρακάτω παρουσιάζεται ο πίνακας διαφοράς των αποτελεσμάτων των 2 αλγορίθμων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(x_train_binary, y_train)\n",
    "print(classification_report(y_test, ada.predict(x_test_binary), zero_division=1))\n",
    "\n",
    "ada_data = classification_data(AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100), x_train_binary, y_train, x_test_binary, y_test, 5)\n",
    "ada_data['estimator'] = 'AdaBoost'\n",
    "ada_table = classification_table(ada_data, x_train_binary)\n",
    "difference_table_ada = abs(ada_table - data_table)\n",
    "difference_table_ada = difference_table_ada.style.set_caption('Πίνακας Διαφοράς για τον {estimator} και τον {estimator_2}'.format(estimator=data['estimator'], estimator_2=ada_data['estimator']))\n",
    "ipd.display(difference_table_ada)\n",
    "\n",
    "classification_plots(ada_data)\n",
    "r = classification_plots_compare(data, ada_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Επιλογή Υπερπαραμέτρων\n",
    "Χρησιμοποιούμε τον παρακάτω κώδικα για να αποφασίσουμε ποιες υπερπαράμετροι είναι καλύτεροι. Σε 3 πίνακες Μ, Κ, Ν, έχουμε διάφορες τιμές με\n",
    "\n",
    "M: αριθμός των λέξεων του λεξιλογίου\n",
    "\n",
    "Ν: αριθμός των πιο συχνών λέξεων για να παραλειφθούν\n",
    "\n",
    "Κ: αριθμός των πιο σπάνιων λέξεων για να παραλειφθούν\n",
    "\n",
    "Χρησιμοποιούμε την ακρίβεια που δίνει ο ταξινομήτης πάνω στα δεδομένα ανάπτυξης για να αποφασίσουμε ποιος συνδυασμός m, n, k είναι ο καλύτερος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N = [50, 100, 200, 300]\n",
    "K = [0, 20, 50, 80]\n",
    "M = [200, 300, 500, 2000, 2500]\n",
    "\n",
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "minK = 0\n",
    "minN = 0\n",
    "minM = 0\n",
    "maxAccuracy = -float('inf')\n",
    "\n",
    "for m in M:\n",
    "    for n in N:\n",
    "        for k in K:\n",
    "            (x_trainn, y_trainn), (x_testt, y_testt) = tf.keras.datasets.imdb.load_data(num_words=m-k, skip_top=n)\n",
    "            x_trainn, x_dev, y_trainn, y_dev = train_test_split(x_trainn, y_trainn, test_size=0.2)\n",
    "            word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "            index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
    "            index2word[0] = '[pad]'\n",
    "            index2word[1] = '[bos]'\n",
    "            index2word[2] = '[oov]'\n",
    "\n",
    "            x_trainn = np.array([' '.join([index2word[idx] for idx in text]) for text in x_trainn])\n",
    "            x_dev = np.array([' '.join([index2word[idx] for idx in text]) for text in x_dev])\n",
    "\n",
    "            x_trainn_binary = binary_vectorizer.fit_transform(x_trainn)\n",
    "            x_val_binary = binary_vectorizer.transform(x_dev)\n",
    "\n",
    "            x_trainn_binary = np.array(x_trainn_binary.toarray())\n",
    "            x_val_binary = np.array(x_val_binary.toarray())\n",
    "            \n",
    "            rfc = RandomForest()\n",
    "            rfc. fit(x_trainn_binary, y_trainn)\n",
    "            predicted_values = rfc.predict(x_val_binary)\n",
    "            print(\"For M={}, N={} and K={}:\".format(m, n, k))\n",
    "            acc = accuracy_score(y_dev, predicted_values)\n",
    "            print('Accuracy:', acc)\n",
    "            if acc > maxAccuracy:\n",
    "                maxAccuracy = acc\n",
    "                minN = n\n",
    "                minK = k\n",
    "                minM = m\n",
    "\n",
    "print(\"\\nBest model parameters are:\\n\\tN: {}\\n\\tK:{} \\n\\tM: {}\".format(minN, minK, minM))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
